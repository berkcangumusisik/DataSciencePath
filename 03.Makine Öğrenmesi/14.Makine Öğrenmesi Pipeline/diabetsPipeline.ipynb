{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grabColNames(dataframe, catTh=10, carTh=20):\n",
    "    \"\"\"\n",
    "\n",
    "    Veri setindeki kategorik, numerik ve kategorik fakat kardinal değişkenlerin isimlerini verir.\n",
    "    Not: Kategorik değişkenlerin içerisine numerik görünümlü kategorik değişkenler de dahildir.\n",
    "\n",
    "    Parameters\n",
    "    ------\n",
    "        dataframe: dataframe\n",
    "                Değişken isimleri alınmak istenilen dataframe\n",
    "        catTh: int, optional\n",
    "                numerik fakat kategorik olan değişkenler için sınıf eşik değeri\n",
    "        carTh: int, optinal\n",
    "                kategorik fakat kardinal değişkenler için sınıf eşik değeri\n",
    "\n",
    "    Returns\n",
    "    ------\n",
    "        catCols: list\n",
    "                Kategorik değişken listesi\n",
    "        numCols: list\n",
    "                Numerik değişken listesi\n",
    "        catButCar: list\n",
    "                Kategorik görünümlü kardinal değişken listesi\n",
    "\n",
    "    Examples\n",
    "    ------\n",
    "        import seaborn as sns\n",
    "        df = sns.load_dataset(\"iris\")\n",
    "        print(grabColNames(df))\n",
    "\n",
    "\n",
    "    Notes\n",
    "    ------\n",
    "        catCols + numCols + catButCar = toplam değişken sayısı\n",
    "        numButCat catCols'un içerisinde.\n",
    "        Return olan 3 liste toplamı toplam değişken sayısına eşittir: catCols + numCols + catButCar = değişken sayısı\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # catCols, catButCar\n",
    "    catCols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n",
    "    numButCat = [col for col in dataframe.columns if dataframe[col].nunique() < catTh and\n",
    "                 dataframe[col].dtypes != \"O\"]\n",
    "    catButCar = [col for col in dataframe.columns if dataframe[col].nunique() > carTh and\n",
    "                 dataframe[col].dtypes == \"O\"]\n",
    "    catCols = catCols + numButCat\n",
    "    catCols = [col for col in catCols if col not in catButCar]\n",
    "\n",
    "    # numCols\n",
    "    numCols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]\n",
    "    numCols = [col for col in numCols if col not in numButCat]\n",
    "\n",
    "    print(f\"Observations: {dataframe.shape[0]}\")\n",
    "    print(f\"Variables: {dataframe.shape[1]}\")\n",
    "    print(f'catCols: {len(catCols)}')\n",
    "    print(f'numCols: {len(numCols)}')\n",
    "    print(f'catButCar: {len(catButCar)}')\n",
    "    print(f'numButCat: {len(numButCat)}')\n",
    "    return catCols, numCols, catButCar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlierThresholds(dataframe, colName, q1=0.25, q3=0.75):\n",
    "    quartile1 = dataframe[colName].quantile(q1)\n",
    "    quartile3 = dataframe[colName].quantile(q3)\n",
    "    interquantileRange = quartile3 - quartile1\n",
    "    upLimit = quartile3 + 1.5 * interquantileRange\n",
    "    lowLimit = quartile1 - 1.5 * interquantileRange\n",
    "    return lowLimit, upLimit\n",
    "def replaceWithThresholds(dataframe, colName):\n",
    "    low, up = outlierThresholds(dataframe, colName)\n",
    "\n",
    "    dataframe.loc[dataframe[colName] > up, colName] = up\n",
    "    dataframe.loc[dataframe[colName] < low, colName] = low\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(dataframe, categoricalCols, drop_first=False):\n",
    "    dataframe = pd.get_dummies(dataframe, columns=categoricalCols, drop_first=drop_first)\n",
    "    return dataframe\n",
    "\n",
    "def diabetes_data_prep(dataframe):\n",
    "    dataframe.columns = [col.upper() for col in dataframe.columns]\n",
    "\n",
    "    # Glucose\n",
    "    dataframe['NEW_GLUCOSECAT'] = pd.cut(x=dataframe['GLUCOSE'], bins=[-1, 139, 200], labels=[\"normal\", \"prediabetes\"])\n",
    "\n",
    "    # Age\n",
    "    dataframe.loc[(dataframe['AGE'] < 35), \"NEW_AGECAT\"] = 'young'\n",
    "    dataframe.loc[(dataframe['AGE'] >= 35) & (dataframe['AGE'] <= 55), \"NEW_AGECAT\"] = 'middleage'\n",
    "    dataframe.loc[(dataframe['AGE'] > 55), \"NEW_AGECAT\"] = 'old'\n",
    "\n",
    "    # BMI\n",
    "    dataframe['NEWBMI_RANGE'] = pd.cut(x=dataframe['BMI'], bins=[-1, 18.5, 24.9, 29.9, 100],\n",
    "                                        labels=[\"underweight\", \"healty\", \"overweight\", \"obese\"])\n",
    "\n",
    "    # BloodPressure\n",
    "    dataframe['NEWBLOODPRESSURE'] = pd.cut(x=dataframe['BLOODPRESSURE'], bins=[-1, 79, 89, 123],\n",
    "                                            labels=[\"normal\", \"hs1\", \"hs2\"])\n",
    "\n",
    "    catCols, numCols, catButCar = grabColNames(dataframe, catTh=5, carTh=20)\n",
    "\n",
    "    catCols = [col for col in catCols if \"OUTCOME\" not in col]\n",
    "\n",
    "    df = one_hot_encoder(dataframe, catCols, drop_first=True)\n",
    "\n",
    "    df.columns = [col.upper() for col in df.columns]\n",
    "\n",
    "    catCols, numCols, catButCar = grabColNames(df, catTh=5, carTh=20)\n",
    "\n",
    "    catCols = [col for col in catCols if \"OUTCOME\" not in col]\n",
    "\n",
    "    replaceWithThresholds(df, \"INSULIN\")\n",
    "\n",
    "    X_scaled = StandardScaler().fit_transform(df[numCols])\n",
    "    df[numCols] = pd.DataFrame(X_scaled, columns=df[numCols].columns)\n",
    "\n",
    "    y = df[\"OUTCOME\"]\n",
    "    X = df.drop([\"OUTCOME\"], axis=1)\n",
    "\n",
    "    return X, y\n",
    "# Base Models\n",
    "def base_models(X, y, scoring=\"roc_auc\"):\n",
    "    print(\"Base Models....\")\n",
    "    classifiers = [('LR', LogisticRegression()),\n",
    "                   ('KNN', KNeighborsClassifier()),\n",
    "                   (\"SVC\", SVC()),\n",
    "                   (\"CART\", DecisionTreeClassifier()),\n",
    "                   (\"RF\", RandomForestClassifier()),\n",
    "                   ('Adaboost', AdaBoostClassifier()),\n",
    "                   ('GBM', GradientBoostingClassifier()),\n",
    "                   ('XGBoost', XGBClassifier(use_label_encoder=False, eval_metric='logloss')),\n",
    "                   ('LightGBM', LGBMClassifier()),\n",
    "                   # ('CatBoost', CatBoostClassifier(verbose=False))\n",
    "                   ]\n",
    "\n",
    "    for name, classifier in classifiers:\n",
    "        cv_results = cross_validate(classifier, X, y, cv=3, scoring=scoring)\n",
    "        print(f\"{scoring}: {round(cv_results['test_score'].mean(), 4)} ({name}) \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_params = {\"n_neighbors\": range(2, 50)}\n",
    "\n",
    "cart_params = {'max_depth': range(1, 20),\n",
    "               \"min_samples_split\": range(2, 30)}\n",
    "\n",
    "rf_params = {\"max_depth\": [8, 15, None],\n",
    "             \"max_features\": [5, 7, \"auto\"],\n",
    "             \"min_samples_split\": [15, 20],\n",
    "             \"n_estimators\": [200, 300]}\n",
    "\n",
    "xgboost_params = {\"learning_rate\": [0.1, 0.01],\n",
    "                  \"max_depth\": [5, 8],\n",
    "                  \"n_estimators\": [100, 200],\n",
    "                  \"colsample_bytree\": [0.5, 1]}\n",
    "\n",
    "lightgbm_params = {\"learning_rate\": [0.01, 0.1],\n",
    "                   \"n_estimators\": [300, 500],\n",
    "                   \"colsample_bytree\": [0.7, 1]}\n",
    "\n",
    "classifiers = [('KNN', KNeighborsClassifier(), knn_params),\n",
    "               (\"CART\", DecisionTreeClassifier(), cart_params),\n",
    "               (\"RF\", RandomForestClassifier(), rf_params),\n",
    "               ('XGBoost', XGBClassifier(use_label_encoder=False, eval_metric='logloss'), xgboost_params),\n",
    "               ('LightGBM', LGBMClassifier(), lightgbm_params)]\n",
    "\n",
    "def hyperparameter_optimization(X, y, cv=3, scoring=\"roc_auc\"):\n",
    "    print(\"Hyperparameter Optimization....\")\n",
    "    best_models = {}\n",
    "    for name, classifier, params in classifiers:\n",
    "        print(f\"########## {name} ##########\")\n",
    "        cv_results = cross_validate(classifier, X, y, cv=cv, scoring=scoring)\n",
    "        print(f\"{scoring} (Before): {round(cv_results['test_score'].mean(), 4)}\")\n",
    "\n",
    "        gs_best = GridSearchCV(classifier, params, cv=cv, n_jobs=-1, verbose=False).fit(X, y)\n",
    "        final_model = classifier.set_params(**gs_best.best_params_)\n",
    "\n",
    "        cv_results = cross_validate(final_model, X, y, cv=cv, scoring=scoring)\n",
    "        print(f\"{scoring} (After): {round(cv_results['test_score'].mean(), 4)}\")\n",
    "        print(f\"{name} best params: {gs_best.best_params_}\", end=\"\\n\\n\")\n",
    "        best_models[name] = final_model\n",
    "    return best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking & Ensemble Learning\n",
    "def voting_classifier(best_models, X, y):\n",
    "    print(\"Voting Classifier...\")\n",
    "    voting_clf = VotingClassifier(estimators=[('KNN', best_models[\"KNN\"]), ('RF', best_models[\"RF\"]),\n",
    "                                              ('LightGBM', best_models[\"LightGBM\"])],\n",
    "                                  voting='soft').fit(X, y)\n",
    "    cv_results = cross_validate(voting_clf, X, y, cv=3, scoring=[\"accuracy\", \"f1\", \"roc_auc\"])\n",
    "    print(f\"Accuracy: {cv_results['test_accuracy'].mean()}\")\n",
    "    print(f\"F1Score: {cv_results['test_f1'].mean()}\")\n",
    "    print(f\"ROC_AUC: {cv_results['test_roc_auc'].mean()}\")\n",
    "    return voting_clf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "İşlem başladı\n",
      "Observations: 768\n",
      "Variables: 13\n",
      "catCols: 5\n",
      "numCols: 8\n",
      "catButCar: 0\n",
      "numButCat: 4\n",
      "Observations: 768\n",
      "Variables: 17\n",
      "catCols: 9\n",
      "numCols: 8\n",
      "catButCar: 0\n",
      "numButCat: 9\n",
      "Base Models....\n",
      "roc_auc: 0.8409 (LR) \n",
      "roc_auc: 0.791 (KNN) \n",
      "roc_auc: 0.8355 (SVC) \n",
      "roc_auc: 0.6321 (CART) \n",
      "roc_auc: 0.8259 (RF) \n",
      "roc_auc: 0.8196 (Adaboost) \n",
      "roc_auc: 0.8249 (GBM) \n",
      "roc_auc: 0.8015 (XGBoost) \n",
      "roc_auc: 0.807 (LightGBM) \n",
      "Hyperparameter Optimization....\n",
      "########## KNN ##########\n",
      "roc_auc (Before): 0.791\n",
      "roc_auc (After): 0.8211\n",
      "KNN best params: {'n_neighbors': 20}\n",
      "\n",
      "########## CART ##########\n",
      "roc_auc (Before): 0.6681\n",
      "roc_auc (After): 0.7943\n",
      "CART best params: {'max_depth': 6, 'min_samples_split': 23}\n",
      "\n",
      "########## RF ##########\n",
      "roc_auc (Before): 0.8233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BERKCAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "c:\\Users\\BERKCAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "c:\\Users\\BERKCAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "c:\\Users\\BERKCAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc (After): 0.8372\n",
      "RF best params: {'max_depth': None, 'max_features': 'auto', 'min_samples_split': 15, 'n_estimators': 300}\n",
      "\n",
      "########## XGBoost ##########\n",
      "roc_auc (Before): 0.8015\n",
      "roc_auc (After): 0.8255\n",
      "XGBoost best params: {'colsample_bytree': 0.5, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}\n",
      "\n",
      "########## LightGBM ##########\n",
      "roc_auc (Before): 0.807\n"
     ]
    }
   ],
   "source": [
    "# Pipeline Main Function\n",
    "################################################\n",
    "\n",
    "def main():\n",
    "    df = pd.read_csv(\"datasets/diabetes.csv\")\n",
    "    X, y = diabetes_data_prep(df)\n",
    "    base_models(X, y)\n",
    "    best_models = hyperparameter_optimization(X, y)\n",
    "    voting_clf = voting_classifier(best_models, X, y)\n",
    "    joblib.dump(voting_clf, \"voting_clf.pkl\")\n",
    "    return voting_clf\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"İşlem başladı\")\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5026d2d6e7aaaaba093def813beadff083d8733e4a511af01735cbabd156995d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
